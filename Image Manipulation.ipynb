{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trans_mat = / 1 0 Tx(move along x axis)/\n",
    "            / 0 1 Ty(move along y axis)/\n",
    "\"\"\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "image = cv2.imread(\"D:/Computer_Vision_Recap/images/input.jpg\")\n",
    "\n",
    "# gets image height and width and div by 4 for Tx and Ty\n",
    "height, width = image.shape[:2]\n",
    "\n",
    "quat_height, quat_width = height/4, width/4\n",
    "\n",
    "# Trans mat\n",
    "T = np.float32([[1, 0, quat_width],\n",
    "               [0, 1, quat_height]])\n",
    "\n",
    "# transformation op\n",
    "img_translated = cv2.warpAffine(image, T, (width, height))\n",
    "cv2.imshow(\"Translated_image\", img_translated)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread(\"D:\\Computer_Vision_Recap\\images/input.jpg\")\n",
    "height, width = image.shape[:2]\n",
    "\n",
    "# creates rotation mat\n",
    "rotation_mat = cv2.getRotationMatrix2D((200, 100), 60, .6)\n",
    "rotated_image = cv2.warpAffine(image, rotation_mat, (width, height))\n",
    "\n",
    "cv2.imshow(\"Rotated_image\", rotated_image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt way to rotate is using transpose func but only rotates in 90 deg\n",
    "img = cv2.imread(\"D:\\Computer_Vision_Recap\\images/input.jpg\")\n",
    "rotated_image = cv2.transpose(img)\n",
    "\n",
    "cv2.imshow(\"Transposed image\", rotated_image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Scaling, Resizing and Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread(\"D:\\Computer_Vision_Recap\\images/input.jpg\")\n",
    "\n",
    "# scaling image to be 65% of original image\n",
    "image_scaled = cv2.resize(image, None, fx=0.65, fy = 0.65)\n",
    "cv2.imshow(\"Scaling-default Linear inter\", image_scaled)\n",
    "cv2.waitKey()\n",
    "\n",
    "# zoom image\n",
    "img_sclaed = cv2.resize(image, None, fx=2.2, fy = 2.2, interpolation = cv2.INTER_CUBIC)\n",
    "cv2.imshow(\"Cubic inter\", img_sclaed)\n",
    "cv2.waitKey()\n",
    "\n",
    "# resizing by setting exact dim\n",
    "image_scaled_2 = cv2.resize(image, (500, 500), cv2.INTER_AREA)\n",
    "cv2.imshow(\"Scaling - Skewed(determined) size\", image_scaled_2)\n",
    "cv2.waitKey()\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "#### Image Pyramids\n",
    "\n",
    "* useful in scaling images in objectg detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread(\"D:\\Computer_Vision_Recap\\images/input.jpg\")\n",
    "\n",
    "half_image = cv2.pyrDown(image)\n",
    "double_image = cv2.pyrUp(image)\n",
    "\n",
    "cv2.imshow(\"Smaller\", half_image)\n",
    "cv2.imshow(\"Larger\", double_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread(\"D:\\Computer_Vision_Recap\\images/input.jpg\")\n",
    "height, width = image.shape[:2]\n",
    "\n",
    "# starting pixel cord(top left of boudning box)\n",
    "start_row, start_col = int(height * .30), int(width * .30)\n",
    "\n",
    "# ending pixel cord(bottom right)\n",
    "end_row, end_col = int(height * .70), int(width * .70)\n",
    "\n",
    "# gets the cropped out rect section fro image\n",
    "cropped_image = image[start_row:end_row, start_col:end_col]\n",
    "\n",
    "cv2.imshow(\"Cropped_image\", cropped_image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Arithmetic operation in opencv \n",
    "* allow us to add(increase) or subtract(decrease) color intensity or brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"D:\\Computer_Vision_Recap\\images/input.jpg\")\n",
    "\n",
    "# matrix used for adding or subtracting color intensities\n",
    "M = np.ones(image.shape, dtype = \"uint8\") * 80\n",
    "\n",
    "# increase color intensity\n",
    "added_image = cv2.add(image, M)\n",
    "cv2.imshow(\"Added_image\", added_image)\n",
    "\n",
    "# decrease color intensity\n",
    "subtracted_image = cv2.subtract(image, M)\n",
    "cv2.imshow(\"Subtracted_image\", subtracted_image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bitwise Operations\n",
    "* does masking stuff and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# makes a square \n",
    "square = np.zeros((300, 300), np.uint8)\n",
    "cv2.rectangle(square, (50, 50), (250, 250), 255, -2)\n",
    "\n",
    "\n",
    "# makes semiCircle\n",
    "semi_circle = np.zeros((300, 300), np.uint8)\n",
    "cv2.ellipse(semi_circle, (150, 150), (150, 150), 30, 0, 180, 255, -2)\n",
    "\n",
    "\n",
    "########## Bitwise Operationws ##########\n",
    "\n",
    "# shows intersection only\n",
    "bit_and = cv2.bitwise_and(square, semi_circle)\n",
    "cv2.imshow(\"bit_and\", bit_and)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# shows either semi_circle or square\n",
    "bit_or = cv2.bitwise_or(square, semi_circle)\n",
    "cv2.imshow(\"bit_or\", bit_or)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# shows where either exists without intersection\n",
    "bit_xor = cv2.bitwise_xor(square, semi_circle)\n",
    "cv2.imshow(\"bit_xor\", bit_xor)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# shows everythin not part of semi_cirle image\n",
    "bit_not = cv2.bitwise_not(semi_circle)\n",
    "cv2.imshow(\"bit_not\", bit_not)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Convolutions and Blurring\n",
    "* Convolution involve passing a kernel over the image and running some on that area to produce and outout image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread(\"D:\\Computer_Vision_Recap\\images/trump.jpg\")\n",
    "\n",
    "# the kernel and the higher the kernel the more the blur effect\n",
    "kernel = np.ones((5, 5), np.float32) / 25\n",
    "\n",
    "# blurring opp\n",
    "blurred_image = cv2.filter2D(image, -1, kernel)\n",
    "cv2.imshow(\"Blurred_image\", blurred_image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### alt commonly used blurring methods\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread(\"D:\\Computer_Vision_Recap\\images/trump.jpg\")\n",
    "\n",
    "# produces harsh blurring effects, and uses box filter\n",
    "avg_blur = cv2.blur(image, (5, 5))\n",
    "cv2.imshow(\"Avg_blur\", avg_blur)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "# used guassian kernel(filter) instead of box filter\n",
    "guassian_blur = cv2.GaussianBlur(image, (5,5), 0)\n",
    "cv2.imshow(\"guassain_blur\", guassian_blur)\n",
    "cv2.waitKey()\n",
    "\n",
    "# takes median of pixels under kernel and central element is (paint type effect)\n",
    "# replaced with median val... bal btw avg and guassian blur\n",
    "median_blur = cv2.medianBlur(image, 5)\n",
    "cv2.imshow(\"Median blur\", median_blur)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "# blurs and remove noise but keeps edge sharp but slow\n",
    "bilateral_blur = cv2.bilateralFilter(image, 5, 75, 75)\n",
    "cv2.imshow(\"Bilateral_blur\", bilateral_blur)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "#### another way of blurring using image Den-noising ######\n",
    "\n",
    "\n",
    "# after none, 6 - filter strenght(5-10) and next if hforColorComponents set to 6 \n",
    "denoised_image = cv2.fastNlMeansDenoisingColored(image, None, 6, 6, 7, 21)\n",
    "cv2.imshow(\"Denoised_image\", denoised_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sharpening\n",
    "* opposite of blurring, stregtens or emphasizes edges in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread(\"D:\\Computer_Vision_Recap\\images/input.jpg\")\n",
    "\n",
    "# sharpening kernel mat, and no normalization needed since equals 1\n",
    "kernel_sharpener = np.array([\n",
    "    [-1, -1, -1],\n",
    "    [-1, 9, -1],\n",
    "    [-1, -1, -1]\n",
    "])\n",
    "\n",
    "# sharping opp\n",
    "edge_sharpened_image = cv2.filter2D(image, -1, kernel_sharpener)\n",
    "cv2.imshow(\"Sharpened_image\", edge_sharpened_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholding, Binarization and Adaptive thresholding\n",
    "\n",
    "**All thresh functions use grayscaled images so images must be conv to grayscaled images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread(\"D:\\Computer_Vision_Recap\\images/trump.jpg\", 0)\n",
    "\n",
    "# pixel below 127 = 0(black) and above = 255(white)\n",
    "ret, thresh_bin = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "cv2.imshow(\"Thresh_bin\", thresh_bin)\n",
    "\n",
    "# pixel below 127 = 255(white) and above = 0(black) i.e reverse of above\n",
    "ret, thresh_bin_inv = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "cv2.imshow(\"Thresh_bin_inv\", thresh_bin_inv)\n",
    "\n",
    "# pixel val above 127 is held there so 255 is off no use\n",
    "ret, thresh_trunc = cv2.threshold(image, 127, 255, cv2.THRESH_TRUNC)\n",
    "cv2.imshow(\"Thresh trunc\", thresh_trunc)\n",
    "\n",
    "# pixel val below 127 = 0 and above are unchanged\n",
    "ret, thresh_tozero = cv2.threshold(image, 127, 255, cv2.THRESH_TOZERO)\n",
    "cv2.imshow(\"Thresh tozero\", thresh_tozero)\n",
    "\n",
    "# reverse of above\n",
    "ret, thresh_tozero_inv = cv2.threshold(image, 127, 255, cv2.THRESH_TOZERO_INV)\n",
    "cv2.imshow(\"Thresh tozero inv\", thresh_tozero_inv)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## alt method of thresholding without specifying the thresh val and is better\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread(\"D:\\Computer_Vision_Recap\\images/sudoku_2.jpg\", 0)\n",
    "\n",
    "# first blur image to reduce any noise\n",
    "image = cv2.GaussianBlur(image, (3,3), 0)\n",
    "\n",
    "\n",
    "# using adaptiveThreshold\n",
    "adapt_thresh = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "                                    cv2.THRESH_BINARY, 3, 5)\n",
    "cv2.imshow(\"Adapt Mean thresh\", adapt_thresh)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "_, otsu_thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "cv2.imshow(\"Otsu Thresholding\", otsu_thresh)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dilation, Erosion, Opening and Closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread(\"D:\\Computer_Vision_Recap\\images/Open.png\")\n",
    "\n",
    "# kernel for performing operation\n",
    "kernel = np.ones((5,5), dtype=\"uint8\")\n",
    "\n",
    "# Erosion\n",
    "erosion = cv2.erode(image, kernel, iterations = 1)\n",
    "cv2.imshow(\"Erosion\", erosion)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# dilation\n",
    "dilation = cv2.dilate(image, kernel, iterations = 1)\n",
    "cv2.imshow(\"Dilation\", dilation)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Opening- gud for removing noise\n",
    "opening = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
    "cv2.imshow(\"Opening\", opening)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Closing - gud for removin noise\n",
    "closing = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "cv2.imshow(\"Closing\", closing)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge Detection and Image Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread(\"D:\\Computer_Vision_Recap\\images/obama.jpg\", 0)\n",
    "\n",
    "height, width = image.shape\n",
    "\n",
    "# Extracts Sobel Edges using sobel algo for X and y\n",
    "sobel_x = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)\n",
    "sobel_y = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=5)\n",
    "\n",
    "# use bitwise_or to merge the results\n",
    "sobel_or = cv2.bitwise_or(sobel_x, sobel_y)\n",
    "cv2.imshow(\"sobel or\", sobel_or)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "# laplacian edge detector algo\n",
    "laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
    "cv2.imshow(\"Laplacian\", laplacian)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "note: In using canny edge detection algo\n",
    "1. image gradient val higher than thresh2 is edge and below thresh1 not edge\n",
    "2. image gradient val btw thresh 1 and thresh2 might be edge or not\n",
    "\"\"\"\n",
    "\n",
    "# the best of the lot\n",
    "canny = cv2.Canny(image, 20, 170)\n",
    "cv2.imshow(\"Canny Egde Detection\", canny)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Perspective Transform of a skewed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms a slanted(skewed image) into it's right form\n",
    "# note: Non Affine transform way\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = cv2.imread(\"D:\\Computer_Vision_Recap\\images/sudoku.jpg\")\n",
    "\n",
    "# gets cordinates of 4 corners\n",
    "points_A = np.float32([\n",
    "    [320, 15],\n",
    "    [300, 215], [85, 100], [59, 90]\n",
    "])\n",
    "\n",
    "points_B = np.float32([\n",
    "    [0, 0],\n",
    "    [420, 0], [0, 594], [420, 594]\n",
    "])\n",
    "\n",
    "# use the two set of four points to compute Perspective T mat\n",
    "M = cv2.getPerspectiveTransform(points_A, points_B)\n",
    "\n",
    "warped_image = cv2.warpPerspective(image, M, (420, 594))\n",
    "cv2.imshow(\"WarpedPerspective Image\", warped_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affine Transform way, needs only 3 points for each set\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = cv2.imread(\"D:\\Computer_Vision_Recap\\images/sudoku.jpg\")\n",
    "height, width, color = image.shape\n",
    "\n",
    "# gets cordinates of 4 corners\n",
    "points_A = np.float32([\n",
    "    [320, 15],\n",
    "    [300, 215], [85, 100]\n",
    "])\n",
    "\n",
    "points_B = np.float32([\n",
    "    [0, 0],\n",
    "    [420, 0], [0, 594]\n",
    "])\n",
    "\n",
    "# use the two set of four points to compute Perspective T mat\n",
    "M = cv2.getAffineTransform(points_A, points_B)\n",
    "\n",
    "warped_image = cv2.warpAffine(image, M, (width, height))\n",
    "cv2.imshow(\"WarpedPerspective Image\", warped_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
